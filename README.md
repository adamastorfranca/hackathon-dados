# Pipeline de Dados ClimÃ¡ticos do INMET (ParaÃ­ba)

![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)
![Framework](https://img.shields.io/badge/pytest-âœ“-green.svg)
![License](https://img.shields.io/badge/license-MIT-purple.svg)

## ğŸ“– VisÃ£o Geral

Este projeto implementa um pipeline de engenharia de dados completo (*End-to-End*) para ingestÃ£o, processamento e transformaÃ§Ã£o de dados climÃ¡ticos histÃ³ricos do **Instituto Nacional de Meteorologia (INMET)**, com foco especÃ­fico nas estaÃ§Ãµes meteorolÃ³gicas do estado da **ParaÃ­ba**.

**Fonte de Dados:** Os dados sÃ£o obtidos oficialmente em [portal.inmet.gov.br/dadoshistoricos](https://portal.inmet.gov.br/dadoshistoricos). O pipeline Ã© configurado para ingerir automaticamente uma janela mÃ³vel dos Ãºltimos 5 anos, comeÃ§ando do ano atual.

**Objetivo:** Fornecer uma base de dados analÃ­tica confiÃ¡vel, auditÃ¡vel e performÃ¡tica para estudos climÃ¡ticos, seguindo as melhores prÃ¡ticas de arquitetura de dados moderna (Arquitetura MedalhÃ£o).

---

## ğŸ“ Sobre o Projeto (Hackathon UNIESP)
Este projeto foi desenvolvido como parte do Hackathon de FinalizaÃ§Ã£o do MBA em Engenharia e CiÃªncia de Dados da UNIESP - Centro UniversitÃ¡rio (JoÃ£o Pessoa - PB).
Coordenador do MBA: Dr. Marcelo Fernandes.
Criador e Avaliador do Hackathon: Pablo Santos (Head de Engenharia de Dados na Radix).
O desafio proposto foi construir uma soluÃ§Ã£o robusta de engenharia de dados que demonstrasse domÃ­nio prÃ¡tico sobre conceitos avanÃ§ados de construÃ§Ã£o de Data Lakes, pipelines de ETL e qualidade de dados.

## ğŸ— Arquitetura do Pipeline
O projeto segue a arquitetura Medallion (Databricks), dividindo os dados em trÃªs camadas lÃ³gicas com nÃ­veis crescentes de qualidade e agregaÃ§Ã£o. Todas as camadas utilizam o formato Parquet com compressÃ£o Snappy, garantindo alta performance de leitura/escrita e eficiÃªncia de armazenamento.

### Detalhamento das Camadas e EstratÃ©gia de Particionamento
A estratÃ©gia de particionamento foi escolhida para otimizar as consultas mais frequentes em cada estÃ¡gio do ciclo de vida do dado.

| Camada | FunÃ§Ã£o | Formato & Particionamento (Performance) | Principais Tratamentos |
|:---:|---|---|---|
| **Bronze** | RepositÃ³rio de dados brutos (Raw), garantindo a reprodutibilidade. | Parquet. Particionado por ano (`partition_year=YYYY`). | Leitura de ZIPs em memÃ³ria, filtro de arquivos de interesse (`_NE_PB_`), conversÃ£o de CSV para Parquet. |
| **Silver** | Dados limpos, enriquecidos, tipados e deduplicados. | Parquet. Particionado por ano e mÃªs (`partition_year=YYYY/partition_month=M`). | NormalizaÃ§Ã£o de colunas (`snake_case`), tipagem forte, tratamento de nulos, conversÃ£o de timezone (`America/Fortaleza`), deduplicaÃ§Ã£o. |
| **Gold** | Dados agregados e prontos para consumo analÃ­tico (BI/ML). | Parquet. Particionado por municÃ­pio (`municipio=NOME`). | CriaÃ§Ã£o de Data Mart com agregaÃ§Ãµes diÃ¡rias (mÃ¡ximas, mÃ­nimas, mÃ©dias), cÃ¡lculo de mÃ©tricas derivadas (ex: amplitude tÃ©rmica). |

## ğŸ“‚ Estrutura do Projeto
A organizaÃ§Ã£o de pastas foi pensada para separar responsabilidades de ingestÃ£o, transformaÃ§Ã£o, orquestraÃ§Ã£o e testes.

```sh
inmet_climate_pipeline/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml              # DefiniÃ§Ã£o do Pipeline de CI/CD (GitHub Actions)
â”œâ”€â”€ data/                   # Armazenamento local dos dados (ignorado pelo git)
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ docs/                   # DocumentaÃ§Ã£o complementar
â”œâ”€â”€ ingestion/              # Scripts de ingestÃ£o (Raw -> Bronze)
â”‚   â””â”€â”€ run_ingestion_bronze.py
â”œâ”€â”€ pipelines/              # Orquestrador do fluxo completo
â”‚   â””â”€â”€ run_all.py
â”œâ”€â”€ tests/                  # SuÃ­te de testes automatizados (UnitÃ¡rios e IntegraÃ§Ã£o)
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ transforms/
â”œâ”€â”€ transforms/             # Scripts de transformaÃ§Ã£o (Bronze->Silver, Silver->Gold)
â”‚   â”œâ”€â”€ run_processing_silver.py
â”‚   â””â”€â”€ run_transformation_gold.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pytest.ini              # ConfiguraÃ§Ã£o do Pytest
â”œâ”€â”€ README.md               # Este arquivo
â””â”€â”€ requirements.txt        # DependÃªncias do projeto
```

## ğŸš€ ComeÃ§ando

Siga os passos abaixo para configurar e executar o projeto em seu ambiente local.

### PrÃ©-requisitos
*   **Python 3.10+**
*   **Git**
*   `pip` (gerenciador de pacotes Python)

### InstalaÃ§Ã£o

1.  **Clone o repositÃ³rio:**
```bash
git clone [https://github.com/adamastorfranca/hackathon-dados.git](https://github.com/adamastorfranca/hackathon-dados.git)
cd hackathon-dados
```

Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

## âš™ï¸ ExecuÃ§Ã£o do Pipeline
VocÃª pode executar o pipeline completo ou cada estÃ¡gio individualmente.
â¤ Pipeline Completo (Recomendado)
Para rodar o fluxo de ponta a ponta (Bronze â†’ Silver â†’ Gold) em sequÃªncia, utilize o script orquestrador:
```bash
python pipelines/run_all.py
```

Este script garante que se uma etapa falhar, as subsequentes nÃ£o serÃ£o iniciadas, mantendo a integridade dos dados.
â¤ ExecuÃ§Ã£o por EstÃ¡gios
Caso precise reprocessar apenas uma camada especÃ­fica:
IngestÃ£o (Raw â†’ Bronze):
Baixa os dados dos Ãºltimos 5 anos e salva em formato bruto.
```bash
python ingestion/run_ingestion_bronze.py
```

Processamento (Bronze â†’ Silver):
LÃª a camada Bronze, aplica limpeza, tipagem e deduplicaÃ§Ã£o.
```bash
python transforms/run_processing_silver.py
```

TransformaÃ§Ã£o (Silver â†’ Gold):
Gera as agregaÃ§Ãµes diÃ¡rias por municÃ­pio para consumo final.
```bash
python transforms/run_transformation_gold.py
```

## ğŸ§ª Testes e Qualidade de CÃ³digo
O projeto adota uma filosofia rigorosa de testes para garantir que alteraÃ§Ãµes no cÃ³digo nÃ£o quebrem o pipeline de dados. Utilizamos o pytest.
Executando os Testes
Para rodar toda a suÃ­te de testes (unitÃ¡rios e de integraÃ§Ã£o):
```bash
pytest
```
